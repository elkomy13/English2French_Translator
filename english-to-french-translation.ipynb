{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:22:36.864314Z","iopub.execute_input":"2025-06-25T12:22:36.864508Z","iopub.status.idle":"2025-06-25T12:23:13.282093Z","shell.execute_reply.started":"2025-06-25T12:22:36.864491Z","shell.execute_reply":"2025-06-25T12:23:13.281495Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59c8e16493445e9af150a01b9117852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"kde4.py:   0%|          | 0.00/4.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf9f6a69d8dc471ea61a6d8daf1a685d"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9774ae3a625a4d96afa8de92ee45509f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/210173 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef3a11aa5da4997a36148076035948b"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"raw_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:23:13.283671Z","iopub.execute_input":"2025-06-25T12:23:13.283934Z","iopub.status.idle":"2025-06-25T12:23:13.288752Z","shell.execute_reply.started":"2025-06-25T12:23:13.283917Z","shell.execute_reply":"2025-06-25T12:23:13.287990Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 210173\n    })\n})"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\nsplit_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:23:13.289421Z","iopub.execute_input":"2025-06-25T12:23:13.289620Z","iopub.status.idle":"2025-06-25T12:23:13.359643Z","shell.execute_reply.started":"2025-06-25T12:23:13.289605Z","shell.execute_reply":"2025-06-25T12:23:13.359114Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 189155\n    })\n    test: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 21018\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"split_datasets[\"validation\"] = split_datasets.pop(\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:23:13.360297Z","iopub.execute_input":"2025-06-25T12:23:13.360531Z","iopub.status.idle":"2025-06-25T12:23:13.364139Z","shell.execute_reply.started":"2025-06-25T12:23:13.360506Z","shell.execute_reply":"2025-06-25T12:23:13.363406Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"split_datasets[\"train\"][\"translation\"][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:23:13.364839Z","iopub.execute_input":"2025-06-25T12:23:13.365048Z","iopub.status.idle":"2025-06-25T12:23:15.718491Z","shell.execute_reply.started":"2025-06-25T12:23:13.365032Z","shell.execute_reply":"2025-06-25T12:23:15.717881Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'en': 'Default to expanded threads',\n 'fr': 'Par défaut, développer les fils de discussion'}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:23:15.719238Z","iopub.execute_input":"2025-06-25T12:23:15.719489Z","iopub.status.idle":"2025-06-25T12:23:30.188397Z","shell.execute_reply.started":"2025-06-25T12:23:15.719463Z","shell.execute_reply":"2025-06-25T12:23:30.187584Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3bd600c9229412291e8c5c87e4d1411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853d5c858f404b5090dd3acc27c72b2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a741ce88774a309937a7b62c9e9872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"002815a697004d838456d6b442e09158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95028b31efa404193ff6a5b7ec6611b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"max_length = 128\n\n\ndef preprocess_function(examples):\n    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n    model_inputs = tokenizer(\n        inputs, text_target=targets, max_length=max_length, truncation=True\n    )\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:23:30.190917Z","iopub.execute_input":"2025-06-25T12:23:30.191340Z","iopub.status.idle":"2025-06-25T12:23:30.195805Z","shell.execute_reply.started":"2025-06-25T12:23:30.191321Z","shell.execute_reply":"2025-06-25T12:23:30.195127Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenized_datasets = split_datasets.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=split_datasets[\"train\"].column_names,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:23:30.196726Z","iopub.execute_input":"2025-06-25T12:23:30.197014Z","iopub.status.idle":"2025-06-25T12:24:09.001707Z","shell.execute_reply.started":"2025-06-25T12:23:30.196990Z","shell.execute_reply":"2025-06-25T12:24:09.000988Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/189155 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54b1d4b93b0419ba59c8aa73fb877f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21018 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0e77c920e249da84f91ef0fda2c3fe"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:09.002523Z","iopub.execute_input":"2025-06-25T12:24:09.002865Z","iopub.status.idle":"2025-06-25T12:24:25.098840Z","shell.execute_reply.started":"2025-06-25T12:24:09.002836Z","shell.execute_reply":"2025-06-25T12:24:25.098134Z"}},"outputs":[{"name":"stderr","text":"2025-06-25 12:24:11.478735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750854251.669298      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750854251.727670      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a33a5d547eb44b995ba86d1c1309e27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9b35064a964f63908274abdbff40e4"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:25.099670Z","iopub.execute_input":"2025-06-25T12:24:25.100325Z","iopub.status.idle":"2025-06-25T12:24:25.152065Z","shell.execute_reply.started":"2025-06-25T12:24:25.100299Z","shell.execute_reply":"2025-06-25T12:24:25.151316Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# we will use the  BLEU score to evaluate the prediction, why BLEU? because The BLEU score evaluates how close the translations are to their labels. It does not measure the intelligibility or grammatical correctness of the model’s generated outputs, but uses statistical rules to ensure that all the words in the generated outputs also appear in the targets. In addition, there are rules that penalize repetitions of the same words if they are not also repeated in the targets (to avoid the model outputting sentences like \"the the the the the\") and output sentences that are shorter than those in the targets (to avoid the model outputting sentences like \"the\").\n\n# One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult to compare scores between models that use different tokenizers. So instead, the most commonly used metric for benchmarking translation models today is SacreBLEU, which addresses this weakness (and others) by standardizing the tokenization step. To use this metric, we first need to install the SacreBLEU library","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:25.152880Z","iopub.execute_input":"2025-06-25T12:24:25.153114Z","iopub.status.idle":"2025-06-25T12:24:29.884317Z","shell.execute_reply.started":"2025-06-25T12:24:25.153097Z","shell.execute_reply":"2025-06-25T12:24:29.883431Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"129c0d8257524d3186933b4b5b4fe5a3"}},"metadata":{}},{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"! pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:29.885421Z","iopub.execute_input":"2025-06-25T12:24:29.886004Z","iopub.status.idle":"2025-06-25T12:24:33.741268Z","shell.execute_reply.started":"2025-06-25T12:24:29.885978Z","shell.execute_reply":"2025-06-25T12:24:33.740140Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.4 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"sacrebleu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:33.742548Z","iopub.execute_input":"2025-06-25T12:24:33.742866Z","iopub.status.idle":"2025-06-25T12:24:35.812129Z","shell.execute_reply.started":"2025-06-25T12:24:33.742838Z","shell.execute_reply":"2025-06-25T12:24:35.811467Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da8f750c950e49dc971059c06c3cef3f"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import numpy as np\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # In case the model returns more than the prediction logits\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100s in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:35.812888Z","iopub.execute_input":"2025-06-25T12:24:35.813687Z","iopub.status.idle":"2025-06-25T12:24:35.818699Z","shell.execute_reply.started":"2025-06-25T12:24:35.813668Z","shell.execute_reply":"2025-06-25T12:24:35.818050Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\nargs = Seq2SeqTrainingArguments(\n    f\"marian-finetuned-kde4-en-to-fr\",\n    eval_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    fp16=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:35.819519Z","iopub.execute_input":"2025-06-25T12:24:35.819696Z","iopub.status.idle":"2025-06-25T12:24:37.948082Z","shell.execute_reply.started":"2025-06-25T12:24:35.819682Z","shell.execute_reply":"2025-06-25T12:24:37.947405Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# from transformers import Seq2SeqTrainer\n\n# trainer = Seq2SeqTrainer(\n#     model,\n#     args,\n#     train_dataset=tokenized_datasets[\"train\"],\n#     eval_dataset=tokenized_datasets[\"validation\"],\n#     data_collator=data_collator,\n#     tokenizer=tokenizer,\n#     compute_metrics=compute_metrics,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:37.948800Z","iopub.execute_input":"2025-06-25T12:24:37.949034Z","iopub.status.idle":"2025-06-25T12:24:37.952477Z","shell.execute_reply.started":"2025-06-25T12:24:37.949017Z","shell.execute_reply":"2025-06-25T12:24:37.951797Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# accelerate","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntokenized_datasets.set_format(\"torch\")\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=8,\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:37.953293Z","iopub.execute_input":"2025-06-25T12:24:37.953612Z","iopub.status.idle":"2025-06-25T12:24:37.973191Z","shell.execute_reply.started":"2025-06-25T12:24:37.953590Z","shell.execute_reply":"2025-06-25T12:24:37.972442Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:37.974003Z","iopub.execute_input":"2025-06-25T12:24:37.974304Z","iopub.status.idle":"2025-06-25T12:24:38.996395Z","shell.execute_reply.started":"2025-06-25T12:24:37.974278Z","shell.execute_reply":"2025-06-25T12:24:38.995856Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:38.997084Z","iopub.execute_input":"2025-06-25T12:24:38.997289Z","iopub.status.idle":"2025-06-25T12:24:39.001778Z","shell.execute_reply.started":"2025-06-25T12:24:38.997272Z","shell.execute_reply":"2025-06-25T12:24:39.001141Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from accelerate import Accelerator\n\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:39.002470Z","iopub.execute_input":"2025-06-25T12:24:39.002687Z","iopub.status.idle":"2025-06-25T12:24:39.288272Z","shell.execute_reply.started":"2025-06-25T12:24:39.002664Z","shell.execute_reply":"2025-06-25T12:24:39.287703Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import get_scheduler\n\nnum_train_epochs = 1\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:39.288971Z","iopub.execute_input":"2025-06-25T12:24:39.289224Z","iopub.status.idle":"2025-06-25T12:24:39.298648Z","shell.execute_reply.started":"2025-06-25T12:24:39.289201Z","shell.execute_reply":"2025-06-25T12:24:39.298046Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def postprocess(predictions, labels):\n    predictions = predictions.cpu().numpy()\n    labels = labels.cpu().numpy()\n\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    return decoded_preds, decoded_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:39.304440Z","iopub.execute_input":"2025-06-25T12:24:39.304631Z","iopub.status.idle":"2025-06-25T12:24:39.309325Z","shell.execute_reply.started":"2025-06-25T12:24:39.304612Z","shell.execute_reply":"2025-06-25T12:24:39.308771Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport torch\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n    # Training\n    model.train()\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n    # Evaluation\n    model.eval()\n    for batch in tqdm(eval_dataloader):\n        with torch.no_grad():\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=128,\n            )\n        labels = batch[\"labels\"]\n\n        # Necessary to pad predictions and labels for being gathered\n        generated_tokens = accelerator.pad_across_processes(\n            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n        )\n        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n\n        predictions_gathered = accelerator.gather(generated_tokens)\n        labels_gathered = accelerator.gather(labels)\n\n        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n\n    results = metric.compute()\n    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n\n    # Save and upload\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T12:24:39.309902Z","iopub.execute_input":"2025-06-25T12:24:39.310177Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/23645 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b556b3787f424cd583791cbc58ccf28a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"save_directory = \"./MARINE-TRANSLATION-EN2FR\"\n\n# Save model\nmodel.save_pretrained(save_directory)\n\n# Save tokenizer\ntokenizer.save_pretrained(save_directory)\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \nprint(f\"Model and tokenizer saved to {save_directory}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}